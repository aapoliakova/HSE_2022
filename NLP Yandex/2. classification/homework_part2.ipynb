{"cells":[{"cell_type":"code","source":["from prettytable import PrettyTable\n","\n","def count_parameters(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        params = parameter.numel()\n","        table.add_row([name, params])\n","        total_params+=params\n","    print(table)\n","    print(f\"Total Trainable Params: {total_params}\")\n","    return total_params\n","  "],"metadata":{"id":"JuW7cF18aQUK","executionInfo":{"status":"ok","timestamp":1658612402260,"user_tz":-60,"elapsed":278,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.simplefilter(\"ignore\", UserWarning)"],"metadata":{"id":"qGVSdZRacw-V","executionInfo":{"status":"ok","timestamp":1658612404436,"user_tz":-60,"elapsed":265,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ITcdWT1yNKAY"},"outputs":[],"source":["!pip install torchdata\n","import torch\n","from torchtext.datasets import AG_NEWS"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"pdpZ1EPSNMUC","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1658612419534,"user_tz":-60,"elapsed":3897,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"103f5777-048d-40a4-f2f8-82cb4ffafc01"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["train_iter = AG_NEWS(split='train')\n","test_iter = AG_NEWS(split='test')\n","batch = [next(iter(train_iter))]\n","y, x = batch[0]\n","x"]},{"cell_type":"code","source":["from torchtext.data import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","import re\n","\n","tokenizer = get_tokenizer('basic_english')\n","train_iter = AG_NEWS(split='train')\n","\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        text = re.sub(r\"[\\\\]\", \" \", text)\n","        text = re.sub(r\"[--]\", \" \", text)\n","        text = re.sub(r\"[=]\", \" \", text)\n","        text = re.sub(r\"[/]\", \" \", text)\n","        yield tokenizer(text)\n","\n","vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<pad>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])\n","\n","print(f\"Words in vocabulary: {len(vocab)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"huBbUksIMJWs","executionInfo":{"status":"ok","timestamp":1658612429115,"user_tz":-60,"elapsed":6462,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"5ca1194f-dfa6-411b-fde2-7ef2b9835804"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Words in vocabulary: 65948\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def text_pipeline(text):\n","    text = re.sub(r\"[\\\\]\", \" \", text)\n","    text = re.sub(r\"[--]\", \" \", text)\n","    text = re.sub(r\"[=]\", \" \", text)\n","    text = re.sub(r\"[/]\", \" \", text)\n","    return tokenizer(text)\n","\n","label_pipeline = lambda x: int(x) - 1\n","\n","\n","def collate_batch(batch):\n","    label_list, text_list = [], []\n","    for (_label, _text) in batch:\n","         label_list.append(label_pipeline(_label))\n","         processed_text = torch.tensor(vocab(text_pipeline(_text)), dtype=torch.int64)\n","         text_list.append(processed_text)\n","         \n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n","    return label_list.to(device), text_list.to(device)\n","\n","train_iter = AG_NEWS(split='train')\n","dataloader = DataLoader(train_iter, batch_size=1, shuffle=False, collate_fn=collate_batch)\n","collate_batch(batch)[1].shape, len(text_pipeline(x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTHEBQrNOsDG","executionInfo":{"status":"ok","timestamp":1658612433419,"user_tz":-60,"elapsed":4306,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"6a369a89-2224-43bb-a49b-8197ea5ec241"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 31]), 31)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import time\n","\n","def train(dataloader):\n","    model.train()\n","    total_acc, total_count = 0, 0\n","    log_interval = 2000\n","    start_time = time.time()\n","\n","    for idx, (label, text) in enumerate(dataloader):\n","        optimizer.zero_grad()\n","        predicted_label = model(text)\n","        loss = criterion(predicted_label, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","        optimizer.step()\n","        total_acc += (predicted_label.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches '\n","                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n","                                              total_acc/total_count))\n","            print(f\"Loss {loss.item()}\")\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","def evaluate(dataloader):\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (label, text) in enumerate(dataloader):\n","            predicted_label = model(text)\n","            loss = criterion(predicted_label, label)\n","            total_acc += (predicted_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc/total_count"],"metadata":{"id":"mCO-P8CSOlXV","executionInfo":{"status":"ok","timestamp":1658612433420,"user_tz":-60,"elapsed":4,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","\n","\n","train_iter, test_iter = AG_NEWS()\n","train_dataset = to_map_style_dataset(train_iter)\n","test_dataset = to_map_style_dataset(test_iter)\n","num_train = int(len(train_dataset) * 0.95)\n","split_train_, split_valid_ = \\\n","    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n","\n","BATCH_SIZE = 64 # batch size for training\n","train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                             shuffle=True, collate_fn=collate_batch)"],"metadata":{"id":"Y2tDt-z3Wnh2","executionInfo":{"status":"ok","timestamp":1658612435520,"user_tz":-60,"elapsed":2104,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["y, X = next(iter(train_dataloader))\n","X.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekxmd8EZZ335","executionInfo":{"status":"ok","timestamp":1658613083619,"user_tz":-60,"elapsed":281,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"be714d24-61d7-4375-80fb-a59f25bc099e"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 104])"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["\n","#### A) Baseline architecture\n","\n","No tricks you know about bag of words.\n","\n"],"metadata":{"id":"434J6MiMY30I"}},{"cell_type":"code","source":["import torch.nn as nn\n","class BagOfEmb(nn.Module):\n","    def __init__(self, n_tokens, embedding_dim):\n","        super(BagOfEmb, self).__init__()\n","        self.embedding_layer = nn.Embedding(n_tokens, embedding_dim, padding_idx=vocab[\"<pad>\"])\n","        self.linear = nn.Linear(embedding_dim, 4)\n","    def forward(self, batch):\n","        out = self.embedding_layer(batch)\n","        out = torch.mean(out, dim=1)\n","        out = self.linear(out)\n","        return out"],"metadata":{"id":"y2DeX7TXPgts","executionInfo":{"status":"ok","timestamp":1658612433420,"user_tz":-60,"elapsed":5,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Models\n","vocab_size = len(vocab)\n","emsize = 25\n","model = BagOfEmb(vocab_size, emsize).to(device)\n","\n","# Hyperparameters\n","EPOCHS = 5 # epoch\n","LR = 5  # learning rate\n","\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","total_accu = None\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","    if total_accu is not None and total_accu > accu_val:\n","      scheduler.step()\n","    else:\n","       total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"riTRdf1SDKWB","executionInfo":{"status":"ok","timestamp":1658535119460,"user_tz":-60,"elapsed":43807,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"5079c2ed-7231-4864-b969-cce84d34eb4c"},"execution_count":103,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------------\n","| end of epoch   1 | time:  8.60s | valid accuracy    0.824 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   2 | time:  8.65s | valid accuracy    0.863 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   3 | time:  8.61s | valid accuracy    0.873 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   4 | time:  8.65s | valid accuracy    0.870 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   5 | time:  8.86s | valid accuracy    0.883 \n","-----------------------------------------------------------\n"]}]},{"cell_type":"code","source":["count_parameters(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49X1eqCwZQ8p","executionInfo":{"status":"ok","timestamp":1658535372214,"user_tz":-60,"elapsed":533,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"6a9b6e20-e451-45af-9760-b8be6cea2b13"},"execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------+------------+\n","|        Modules         | Parameters |\n","+------------------------+------------+\n","| embedding_layer.weight |  1648700   |\n","|     linear.weight      |    100     |\n","|      linear.bias       |     4      |\n","+------------------------+------------+\n","Total Trainable Params: 1648804\n"]},{"output_type":"execute_result","data":{"text/plain":["1648804"]},"metadata":{},"execution_count":111}]},{"cell_type":"markdown","source":["\n","#### A) CNN architecture\n","\n","All the tricks you know about dense and convolutional neural networks apply here as well.\n","* Dropout. Nuff said.\n","* Batch Norm. This time it's `nn.BatchNorm*`/`L.BatchNormalization`\n","* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n","* More layers, more neurons, ya know...\n","\n","![picture](https://lena-voita.github.io/resources/lectures/text_clf/neural/cnn/several_kernel_sizes-min.png)"],"metadata":{"id":"rTkwsD2qY14b"}},{"cell_type":"code","source":["class ParalellConv(nn.Module):\n","      def __init__(self, embedding_dim, n_filters, kernel_size):\n","        super(ParalellConv, self).__init__()\n","        # Conv - RelU - Droput - MaxOverTime\n","        self.layers = nn.Sequential(\n","            nn.Conv1d(embedding_dim, n_filters, kernel_size), \n","            nn.ReLU(),\n","            nn.Dropout(p=0.1)\n","        )\n","      def forward(self, x):\n","          return self.layers(x)\n","\n","\n","class CNN_TEXT(nn.Module):\n","    def __init__(self, n_tokens, embedding_dim):\n","        super(CNN_TEXT, self).__init__()\n","        self.embedding_layer = nn.Embedding(n_tokens, embedding_dim)\n","        self.conv2 = ParalellConv(embedding_dim, 16, kernel_size=2)\n","        self.conv4 = ParalellConv(embedding_dim, 8, kernel_size=3)\n","        self.conv6 = ParalellConv(embedding_dim, 4, kernel_size=4)\n","\n","        self.bn = nn.BatchNorm1d(28)\n","        self.linear = nn.Linear(28, 4)\n","\n","    def forward(self, batch):\n","        emb_out = self.embedding_layer(batch)\n","        emb_perm = torch.permute(emb_out, (0, 2, 1))\n","\n","        out2 = self.conv2(emb_perm).max(-1)[0]\n","        out4 = self.conv4(emb_perm).max(-1)[0]\n","        out6 = self.conv6(emb_perm).max(-1)[0]\n","        out = torch.cat([out2, out4, out6], axis=1)\n","        out = self.bn(out)\n","        out = self.linear(out)\n","\n","        return out"],"metadata":{"id":"BtFOuzGPXayI","executionInfo":{"status":"ok","timestamp":1658594685185,"user_tz":-60,"elapsed":305,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Models\n","vocab_size = len(vocab)\n","emsize = 25\n","model = CNN_TEXT(vocab_size, emsize).to(device)\n","\n","# Hyperparameters\n","EPOCHS = 5 # epoch\n","LR = 3  # learning rate\n","\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","total_accu = None\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","    if total_accu is not None and total_accu > accu_val:\n","      scheduler.step()\n","    else:\n","       total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSct4mVOVNCm","executionInfo":{"status":"ok","timestamp":1658536476010,"user_tz":-60,"elapsed":63075,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"935c46f0-0cb2-44a1-bd67-fa38a5206d7f"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------------\n","| end of epoch   1 | time: 12.56s | valid accuracy    0.709 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 12.62s | valid accuracy    0.775 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 12.49s | valid accuracy    0.790 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   4 | time: 12.51s | valid accuracy    0.812 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   5 | time: 12.51s | valid accuracy    0.823 \n","-----------------------------------------------------------\n"]}]},{"cell_type":"code","source":["count_parameters(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIzxCQLeaZ0e","executionInfo":{"status":"ok","timestamp":1658536483217,"user_tz":-60,"elapsed":257,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"5c3d1630-c61f-409e-932b-c6e2a647e516"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------+------------+\n","|        Modules         | Parameters |\n","+------------------------+------------+\n","| embedding_layer.weight |  1648700   |\n","| conv2.layers.0.weight  |    800     |\n","|  conv2.layers.0.bias   |     16     |\n","| conv4.layers.0.weight  |    600     |\n","|  conv4.layers.0.bias   |     8      |\n","| conv6.layers.0.weight  |    400     |\n","|  conv6.layers.0.bias   |     4      |\n","|       bn.weight        |     28     |\n","|        bn.bias         |     28     |\n","|     linear.weight      |    112     |\n","|      linear.bias       |     4      |\n","+------------------------+------------+\n","Total Trainable Params: 1650700\n"]},{"output_type":"execute_result","data":{"text/plain":["1650700"]},"metadata":{},"execution_count":125}]},{"cell_type":"markdown","source":["#### B) Play with pooling\n","\n","There's more than one way to perform pooling:\n","* Max over time (independently for each feature)\n","* Average over time (excluding PAD)\n","* Softmax-pooling:\n","$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n","\n","* Attentive pooling\n","$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n","\n",", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n","and $NN_{attn}$ is a dense layer.\n","\n","The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$ (aka multi-headed attention).\n","\n","The catch is that keras layers do not inlude those toys. You will have to [write your own keras layer](https://keras.io/layers/writing-your-own-keras-layers/). Or use pure tensorflow, it might even be easier :)"],"metadata":{"id":"UHyjh_eRfLju"}},{"cell_type":"code","source":["### SKIP DIFFERENT POOLING "],"metadata":{"id":"dSzo_fCji30u","executionInfo":{"status":"ok","timestamp":1658593778280,"user_tz":-60,"elapsed":271,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#### C) Fun with words\n","\n","It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n","\n","* Use a pre-trained embeddings from `gensim.downloader.load`. See last lecture.\n","* Start with pre-trained embeddings, then fine-tune them with gradient descent. You may or may not download pre-trained embeddings from [here](http://nlp.stanford.edu/data/glove.6B.zip) and follow this [manual](https://keras.io/examples/nlp/pretrained_word_embeddings/) to initialize your Pytorch embedding layer with downloaded weights."],"metadata":{"id":"RW5oPh_55KKX"}},{"cell_type":"markdown","source":["Initialize embedding matrix with glove embeddings, \n","for out of vocabulary use random initialization"],"metadata":{"id":"2ApTeBVe6fTA"}},{"cell_type":"code","source":["import gensim.downloader as api\n","import numpy as np\n","\n","\n","glove = api.load(\"glove-twitter-25\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eVP-MDkF6eOw","executionInfo":{"status":"ok","timestamp":1658612504183,"user_tz":-60,"elapsed":51211,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"d8dfef92-1e51-417d-993d-0f4067f0295c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 104.8/104.8MB downloaded\n"]}]},{"cell_type":"code","source":["embeddings_matrix = np.zeros((len(vocab), 25))\n","embeddings_dim = glove[\"will\"].shape\n","\n","for word, index in vocab.get_stoi().items():\n","    if word in glove:\n","        embeddings_matrix[index] = glove.get_vector(word)\n","    else:\n","        embeddings_matrix[index] = np.random.uniform(-0.5, 0.5, embeddings_dim)\n","\n","embeddings_matrix = torch.tensor(embeddings_matrix, device=device, dtype=torch.float32)\n","print(f\"Sanity check n_words, word_shape {embeddings_matrix.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CDijKxv4-cV","executionInfo":{"status":"ok","timestamp":1658612810246,"user_tz":-60,"elapsed":386,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"89894341-9661-4498-d464-f0459ca5e117"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Sanity check n_words, word_shape torch.Size([65948, 25])\n"]}]},{"cell_type":"code","source":["class CNN_TEXT(nn.Module):\n","    def __init__(self, embeddings_matrix, embedding_dim, freeze=False):\n","        super(CNN_TEXT, self).__init__()\n","        self.embedding_layer = nn.Embedding.from_pretrained(embeddings_matrix, freeze=freeze).to(device)\n","        self.conv2 = ParalellConv(embedding_dim, 16, kernel_size=2)\n","        self.conv4 = ParalellConv(embedding_dim, 8, kernel_size=3)\n","        self.conv6 = ParalellConv(embedding_dim, 4, kernel_size=4)\n","\n","        self.bn = nn.BatchNorm1d(28)\n","        self.linear = nn.Linear(28, 4)\n","\n","    def forward(self, batch):\n","        emb_out = self.embedding_layer(batch)\n","        emb_perm = torch.permute(emb_out, (0, 2, 1))\n","\n","        out2 = self.conv2(emb_perm).max(-1)[0]\n","        out4 = self.conv4(emb_perm).max(-1)[0]\n","        out6 = self.conv6(emb_perm).max(-1)[0]\n","        out = torch.cat([out2, out4, out6], axis=1)\n","        out = self.bn(out)\n","        out = self.linear(out)\n","\n","        return out"],"metadata":{"id":"02OuTS6s7vhx","executionInfo":{"status":"ok","timestamp":1658595784374,"user_tz":-60,"elapsed":288,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["# Models\n","emsize = 25\n","model = CNN_TEXT(embeddings_matrix, emsize).to(device)\n","\n","# Hyperparameters\n","EPOCHS = 5 # epoch\n","LR = 3  # learning rate\n","\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","total_accu = None\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","    if total_accu is not None and total_accu > accu_val:\n","      scheduler.step()\n","    else:\n","       total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WL1FFc0J8PjH","executionInfo":{"status":"ok","timestamp":1658595912712,"user_tz":-60,"elapsed":124636,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"0ebe447b-7bfb-4b59-f629-f85cc377f27e"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------------\n","| end of epoch   1 | time: 25.36s | valid accuracy    0.879 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 24.79s | valid accuracy    0.892 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 25.20s | valid accuracy    0.901 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   4 | time: 24.82s | valid accuracy    0.903 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   5 | time: 24.11s | valid accuracy    0.900 \n","-----------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["\n","#### D) Going recurrent\n","\n","We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n","\n","* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n","* Since you know all the text in advance, use bidirectional RNN\n","  * Run one LSTM from left to right\n","  * Run another in parallel from right to left \n","  * Concatenate their output sequences along unit axis (dim=-1)\n","\n","![picture](https://lena-voita.github.io/resources/lectures/text_clf/neural/rnn/rnn_final_state-min.png)\n","\n"],"metadata":{"id":"isashhOLBow1"}},{"cell_type":"code","source":["class LSTM(nn.Module):\n","    def __init__(self, freeze=False):\n","        super(LSTM, self).__init__()\n","        self.embedding_layer = nn.Embedding.from_pretrained(embeddings_matrix, freeze=freeze).to(device)\n","        self.lstm = nn.LSTM(input_size=25,\n","                    hidden_size=50,\n","                    num_layers=1,\n","                    batch_first=True,\n","                    bidirectional=False)\n","        self.drop = nn.Dropout(p=0.5)\n","        self.linear = nn.Linear(50, 4)\n","        \n","    def forward(self, batch):\n","        emb_out = self.embedding_layer(batch)\n","        lstm_out, (last_hidden, _) = self.lstm(emb_out)\n","        out = self.drop(torch.squeeze(last_hidden, 0))\n","        out = self.linear(out)\n","        return out "],"metadata":{"id":"XlZbQvm8Cinq","executionInfo":{"status":"ok","timestamp":1658614589694,"user_tz":-60,"elapsed":261,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["# Models\n","emsize = 25\n","model = LSTM().to(device)\n","\n","# Hyperparameters\n","EPOCHS = 5 # epoch\n","LR = 3  # learning rate\n","\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","total_accu = None\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","    if total_accu is not None and total_accu > accu_val:\n","      scheduler.step()\n","    else:\n","       total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mhz2INJqGWlY","executionInfo":{"status":"ok","timestamp":1658614681315,"user_tz":-60,"elapsed":51734,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"f52551fd-15b3-4620-ce2b-1541f90b17fd"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------------\n","| end of epoch   1 | time: 10.68s | valid accuracy    0.245 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 10.07s | valid accuracy    0.578 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 10.16s | valid accuracy    0.861 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   4 | time: 10.31s | valid accuracy    0.859 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   5 | time: 10.13s | valid accuracy    0.891 \n","-----------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["![picture](https://lena-voita.github.io/resources/lectures/text_clf/neural/rnn/bidirectional-min.png)"],"metadata":{"id":"-OfgiJm9MThC"}},{"cell_type":"code","source":["class LSTM_Bi(nn.Module):\n","    def __init__(self, freeze=False):\n","        super(LSTM_Bi, self).__init__()\n","        self.embedding_layer = nn.Embedding.from_pretrained(embeddings_matrix, freeze=freeze).to(device)\n","        self.lstm = nn.LSTM(input_size=25,\n","                    hidden_size=50,\n","                    num_layers=1,\n","                    batch_first=True,\n","                    bidirectional=True)\n","        self.drop = nn.Dropout(p=0.5)\n","        self.linear = nn.Linear(2 * 50, 4)\n","        \n","    def forward(self, batch):\n","        emb_out = self.embedding_layer(batch)\n","        lstm_out, (last_hidden, _) = self.lstm(emb_out)\n","        hidden_concat = torch.cat([last_hidden[0], last_hidden[1]], dim=1)\n","        out = self.drop(hidden_concat)\n","        out = self.linear(out)\n","        return  out"],"metadata":{"id":"IYuzDoaWIpmj","executionInfo":{"status":"ok","timestamp":1658615461652,"user_tz":-60,"elapsed":2,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["# Models\n","emsize = 25\n","model = LSTM_Bi().to(device)\n","\n","# Hyperparameters\n","EPOCHS = 5 # epoch\n","LR = 3  # learning rate\n","\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","total_accu = None\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","    if total_accu is not None and total_accu > accu_val:\n","      scheduler.step()\n","    else:\n","       total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xQAEzKhJCwT","executionInfo":{"status":"ok","timestamp":1658615521600,"user_tz":-60,"elapsed":58163,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"b6652b32-0b57-4396-e59d-cbd07ea3ada4"},"execution_count":99,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------------\n","| end of epoch   1 | time: 11.80s | valid accuracy    0.893 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 11.97s | valid accuracy    0.898 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 11.60s | valid accuracy    0.904 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   4 | time: 11.42s | valid accuracy    0.908 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   5 | time: 11.05s | valid accuracy    0.911 \n","-----------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# sum_lasthidden = output[:, -1, :hidden_size] + output[:, -1, hidden_size:]\n","# https://stackoverflow.com/questions/50856936/taking-the-last-state-from-bilstm-bigru-in-pytorch\n","\n","class LSTM_Bi(nn.Module):\n","    def __init__(self, freeze=False):\n","        super(LSTM_Bi, self).__init__()\n","        self.embedding_layer = nn.Embedding.from_pretrained(embeddings_matrix, freeze=freeze).to(device)\n","        self.lstm = nn.LSTM(input_size=25,\n","                    hidden_size=50,\n","                    num_layers=1,\n","                    batch_first=True,\n","                    bidirectional=True)\n","        self.drop = nn.Dropout(p=0.5)\n","        self.linear = nn.Linear(2 * 50, 4)\n","        \n","    def forward(self, batch):\n","        emb_out = self.embedding_layer(batch)\n","        output, (last_hidden, _) = self.lstm(emb_out)\n","        hidden_size = 50\n","        hidden_concat = torch.cat([output[:, -1, :hidden_size], output[:, -1, hidden_size:]], dim=1)\n","        out = self.drop(hidden_concat)\n","        out = self.linear(out)\n","        return  out"],"metadata":{"id":"6zj3bcxcJKZX","executionInfo":{"status":"ok","timestamp":1658617752775,"user_tz":-60,"elapsed":279,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":["# Models\n","emsize = 25\n","model = LSTM_Bi().to(device)\n","\n","# Hyperparameters\n","EPOCHS = 5 # epoch\n","LR = 3  # learning rate\n","\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","total_accu = None\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","    if total_accu is not None and total_accu > accu_val:\n","      scheduler.step()\n","    else:\n","       total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFNrmPpDUmkz","executionInfo":{"status":"ok","timestamp":1658617822642,"user_tz":-60,"elapsed":58491,"user":{"displayName":"Poliakova A Anna","userId":"04844337654915059046"}},"outputId":"3ef92aed-3be7-4082-c923-492331a887e0"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------------\n","| end of epoch   1 | time: 11.84s | valid accuracy    0.254 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 11.67s | valid accuracy    0.887 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 11.67s | valid accuracy    0.892 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   4 | time: 11.57s | valid accuracy    0.895 \n","-----------------------------------------------------------\n","-----------------------------------------------------------\n","| end of epoch   5 | time: 11.46s | valid accuracy    0.906 \n","-----------------------------------------------------------\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"aFgkuPf5Upbe"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"homework2.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNpuNNlkWjZEXSCXZmbX40g"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}